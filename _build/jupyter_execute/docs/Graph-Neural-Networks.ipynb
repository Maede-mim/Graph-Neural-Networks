{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64c8f97",
   "metadata": {},
   "source": [
    "![](images/fum-gnn-logo.png)\n",
    "%%HTML\n",
    "<script src=\"require.js\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024683b",
   "metadata": {},
   "source": [
    "# Graph-Neural-Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da439c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Graph is an expressive and powerful data structure that is widely applicable, due to its fexibility and effectiveness in modeling and representing graph structure data. It has been more and more popular in various felds, including biology,\n",
    "fnance, transportation, social network, among many others.\n",
    "Graph Neural Networks are neural architectures specifcally designed for\n",
    "graph-structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dac1c7",
   "metadata": {},
   "source": [
    "# Basics of graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728988df",
   "metadata": {},
   "source": [
    "- **Graph**: A graph is composed of a node set and an edge set, where nodes represent entities and edges represent the relationship between entities. The nodes and edges form the topology structure of the graph. Besides the graph structure,nodes, edges, and/or the whole graph can be associated with rich information represented as node/edge/graph features (also known as attributes or contents).\n",
    "for a molcule feature vector would contain information about atoms , for a social network attributes about a person for instans .\n",
    "\n",
    "![](images/Asprin-grah.png)\n",
    "%%HTML\n",
    "<script src=\"require.js\"></script>\n",
    "\n",
    "- **Adjancy Mtaric**: it tells us nodes are connected\n",
    "\n",
    "![](images/Adajency_matrice.png)\n",
    "%%HTML\n",
    "<script src=\"require.js\"></script>\n",
    "\n",
    "- **Subgraph**: A subgraph is a graph whose set of nodes and set of edges are all subsets of the original graph\n",
    "\n",
    "- **Heterogeneous Graphs**: Graphs are called heterogeneous if the nodes and/or edges of the graph are from different types. A typical example of heteronomous graphs is knowledge graphs where the edges are composed of different types\n",
    "\n",
    "- **Dynamic Graph**: refers to when at least one component of the\n",
    "graph data changes over time, e.g., adding or deleting nodes, adding or deleting\n",
    "edges, changing edges weights or changing node attributes, etc. If graphs are\n",
    "not dynamic, we refer to them as static graphs.\n",
    "\n",
    "- **Undirected Graphs**: In these graphs, edges are bidirectional, representing two-way relationships without any direction.\n",
    "\n",
    "![](images/Undirected-graph.png)\n",
    "%%HTML\n",
    "<script src=\"require.js\"></script>\n",
    "\n",
    "- **Directed Graphs**: In these graphs, edges have a direction, indicated by an arrow, showing a one-way relationship from one node to another.\n",
    "\n",
    "![](images/Directed_graph.png)\n",
    "%%HTML\n",
    "<script src=\"require.js\"></script>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e2775",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Graph Representation Learning\n",
    "\n",
    "graph representation learning aims at assigning nodes in a graph to low-dimensional representations and effectively preserving the graph structures.\n",
    "To process graph data effectively,\n",
    "the frst critical challenge is to fnd effective graph data representation, that is, how\n",
    "to represent graphs concisely so that advanced analytic tasks, such as pattern discovery, analysis, and prediction, can be conducted effciently in both time and space.\n",
    "\n",
    "### Challenges of Traditional Graph Representation\n",
    "\n",
    "The traditional graph representation is defined as:\n",
    "\n",
    "$$\n",
    "G = (V, E)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $V$ represents the set of nodes (vertices)\n",
    "- $E$ represents the set of edges\n",
    "\n",
    "This representation faces several challenges when dealing with **large-scale graphs** (billions of nodes), including:\n",
    "\n",
    "1. **High Computational Complexity**: \n",
    "   - Operations like traversals scale as $O(|V| + |E|)$\n",
    "   - Storage requirements grow as $O(|V|^2)$ for dense graphs\n",
    "\n",
    "2. **Low Parallelizability**:\n",
    "   - Many graph algorithms (e.g., DFS) have inherent sequential dependencies\n",
    "   - Poor cache locality for irregular graph structures\n",
    "\n",
    "3. **Incompatibility with Machine Learning**:\n",
    "   - Non-Euclidean structure requires special adaptation for DL\n",
    "   - Traditional representations don't naturally support gradient-based optimization\n",
    "   - Difficulty in learning meaningful node/edge embeddings\n",
    "\n",
    "\n",
    "To address these challenges, graph representation learning has emerged, which aims to encode nodes into dense, low-dimensional vectors that preserve structural information. The goals are to reconstruct the original graph from these embeddings and facilitate downstream inference tasks such as link prediction, node classification, and clustering.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Traditional Graph Embedding**\n",
    "\n",
    "\n",
    "##  **Modern Graph Embedding**\n",
    "\n",
    "### 1. Structure-Property Preserving Graph Representation Learning\n",
    "        ''' Among all the information encoded in a graph, graph structures and properties are  two crucial factors that largely affect graph inference.'''\n",
    "- ####  1.1 Structure Preserving Graph Representation Learning\n",
    "        \n",
    "- ####  1.2 Property Preserving Graph Representation Learning\n",
    "\n",
    "### 2. Graph Representation Learning with Side Information\n",
    "- ####  2.1 Advanced Information Preserving Graph Representation Learning\n",
    "\n",
    "### Graph Neural Networks\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b7903",
   "metadata": {},
   "source": [
    "##  Graph Neural Networks: An Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56adb783",
   "metadata": {},
   "source": [
    "Deep learning has revolutionized AI, excelling in Euclidean data like images and sequences. However, many real-world problems involve complex structures like graphs and manifolds—seen in social networks, recommendation systems, and drug discovery—that traditional models struggle to capture. Recently, Graph Neural Networks (GNNs) have gained significant attention for effectively modeling such graph-structured data, achieving remarkable success across various fields including bioinformatics, NLP, computer vision, and urban analytics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f46395",
   "metadata": {},
   "source": [
    "# Core Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c904e94",
   "metadata": {},
   "source": [
    "### Node Representation Update\n",
    "GNNs update node representations by aggregating features from neighbors:\n",
    "\n",
    "$\n",
    "\\mathbf{h}_v^{(k)} = \\text{UPDATE}^{(k)} \\left( \\mathbf{h}_v^{(k-1)}, \\; \\text{AGGREGATE}^{(k)} \\left( \\left\\{ \\mathbf{h}_u^{(k-1)} : u \\in \\mathcal{N}(v) \\right\\} \\right) \\right)\n",
    "$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- $\\mathbf{h}_v^{(k)}$: node embedding at layer $k$\n",
    "- $\\mathcal{N}(v)$: neighbors of node $v$\n",
    "- $\\text{AGGREGATE}$: aggregation function (mean, sum, max)\n",
    "- $\\text{UPDATE}$: typically a neural network\n",
    "---\n",
    "\n",
    "### **Graph Convolutional Network (GCN)**\n",
    "The layer update rule:\n",
    "\n",
    "$\n",
    "\\mathbf{H}^{(k)} = \\sigma \\left( \\tilde{\\mathbf{D}}^{-\\frac{1}{2}} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-\\frac{1}{2}} \\mathbf{H}^{(k-1)} \\mathbf{W}^{(k)} \\right)\n",
    "$\n",
    "\n",
    "**Where:**\n",
    "- $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$: adjacency matrix with added self-loops\n",
    "- $\\tilde{\\mathbf{D}}$: degree matrix of $\\tilde{\\mathbf{A}}$\n",
    "- $\\mathbf{W}^{(k)}$: learnable weights\n",
    "- $\\sigma$: activation function (e.g., ReLU)\n",
    "\n",
    "---\n",
    "\n",
    "### **Graph Attention Network (GAT)**\n",
    "Attention coefficients:\n",
    "\n",
    "$\n",
    "\\alpha_{uv} = \\frac{\n",
    "\\exp \\left( \\text{LeakyReLU} \\left( \\mathbf{a}^\\top \\left[ \\mathbf{W} \\mathbf{h}_u \\; \\| \\; \\mathbf{W} \\mathbf{h}_v \\right] \\right) \\right)\n",
    "}{\n",
    "\\sum_{k \\in \\mathcal{N}(v)} \\exp \\left( \\text{LeakyReLU} \\left( \\mathbf{a}^\\top \\left[ \\mathbf{W} \\mathbf{h}_k \\; \\| \\; \\mathbf{W} \\mathbf{h}_v \\right] \\right) \\right)\n",
    "}\n",
    "$ \n",
    "\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "- **$\\mathbf{h}_u, \\mathbf{h}_v$:** Original feature vectors of nodes $u$ and $v$.\n",
    "- **$\\mathbf{W}$:** Learnable weight matrix transforming features into a suitable space.\n",
    "- **Concatenation $\\left[ \\mathbf{W} \\mathbf{h}_u \\; \\| \\; \\mathbf{W} \\mathbf{h}_v \\right]$:** Combines features of nodes $u$ and $v$.\n",
    "- **$\\mathbf{a}$:** Attention vector, learned during training, projecting concatenated features into a scalar score.\n",
    "- **LeakyReLU:** Activation that introduces non-linearity.\n",
    "- **Exponentiation $\\exp()$:** Converts scores into positive values.\n",
    "- **Normalization (denominator):** Summing over all neighbors' exponentiated scores ensures that weights sum to 1, like a probability distribution.\n",
    "\n",
    "This process results in normalized attention weights (\\(\\alpha_{uv}\\)) that determine how much influence each neighboring node has during feature aggregation in GAT.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Node update:\n",
    "\n",
    "$\n",
    "\\boxed{\n",
    "\\mathbf{h}_v' = \\sigma \\left( \\sum_{u \\in \\mathcal{N}(v)} \\alpha_{uv} \\mathbf{W} \\mathbf{h}_u \\right)\n",
    "}\n",
    "$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}