{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a586a5",
   "metadata": {},
   "source": [
    "![](images/CausalGraphLearning.png)\n",
    "%%HTML\n",
    "<script src=\"require.js\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505d065",
   "metadata": {},
   "source": [
    "# DAGs with NOTEATS Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76703f7",
   "metadata": {},
   "source": [
    "In this lecture ,we're going to have a detailable review on DAGs with NOTEARS papre üîé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462f618",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b142c",
   "metadata": {},
   "source": [
    "Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian\n",
    "networks) is a challenging problem since the search space of DAGs is combinatorial\n",
    "and scales superexponentially with the number of nodes. Existing approaches\n",
    "rely on various local heuristics for enforcing the acyclicity constraint. In this\n",
    "paper, we introduce a fundamentally different strategy: we formulate the structure\n",
    "learning problem as a purely continuous optimization problem over real matrices\n",
    "that avoids this combinatorial constraint entirely. This is achieved by a novel\n",
    "characterization of acyclicity that is not only smooth but also exact. The resulting\n",
    "problem can be efficiently solved by standard numerical algorithms, which also\n",
    "makes implementation effortless. The proposed method outperforms existing\n",
    "ones, without imposing any structural assumptions on the graph such as bounded\n",
    "treewidth or in-degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b0fab",
   "metadata": {},
   "source": [
    "# Learning Directed Acyclic Graphs with Continuous Optimization\n",
    "\n",
    "Learning directed acyclic graphs (DAGs) from data is an NP-hard problem [8, 11], owing mainly to the combinatorial acyclicity constraint that is difficult to enforce efficiently. At the same time, DAGs are popular models in practice, with applications in biology [33], genetics [49], machine learning [22], and causal inference [42]. For this reason, the development of new methods for learning DAGs remains a central challenge in machine learning and statistics.\n",
    "\n",
    "## Continuous Optimization Approach\n",
    "\n",
    "In this paper, we propose a new approach for score-based learning of DAGs by converting the traditional combinatorial optimization problem (left) into a continuous program (right):\n",
    "\n",
    "$$\n",
    "\\min_{W\\in\\mathbb{R}^{d\\times d}} F(W) \\text{ subject to } G(W) \\in \\text{DAGs} \\quad \\Leftrightarrow \\quad \\min_{W\\in\\mathbb{R}^{d\\times d}} F(W) \\text{ subject to } h(W)=0\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $G(W)$ is the $d$-node graph induced by the weighted adjacency matrix $W$\n",
    "- $F: \\mathbb{R}^{d\\times d} \\to \\mathbb{R}$ is a score function (see Section 2.1 for details)\n",
    "- $h: \\mathbb{R}^{d\\times d} \\to \\mathbb{R}$ is our key technical device: a smooth function over real matrices whose level set at zero exactly characterizes acyclic graphs\n",
    "\n",
    "## Advantages\n",
    "\n",
    "Although the two problems are equivalent, the continuous program on the right:\n",
    "- Eliminates the need for specialized algorithms tailored to search over the combinatorial space of DAGs\n",
    "- Allows leveraging standard numerical algorithms for constrained problems\n",
    "- Makes implementation particularly easy, not requiring any knowledge about graphical models\n",
    "\n",
    "This is similar in spirit to the situation for undirected graphical models, where the formulation of a continuous log-det program [4] sparked remarkable advances in structure learning for undirected graphs (Section 2.2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d643adb1",
   "metadata": {},
   "source": [
    "## Contributions\n",
    "\n",
    "The main thrust of this work is to re-formulate score-based learning of DAGs so that standard smooth optimization schemes such as L-BFGS [28] can be leveraged. To accomplish this, we make the following specific contributions:\n",
    "\n",
    "- **Explicit smooth acyclicity constraint**:  \n",
    "  We explicitly construct a smooth function over ‚Ñù<sup>d√ód</sup> with computable derivatives that encodes the acyclicity constraint. This allows us to replace the combinatorial constraint *G ‚àà D* with a smooth equality constraint.\n",
    "\n",
    "- **Equality-constrained program for DAG learning**:  \n",
    "  We develop an equality-constrained program for simultaneously estimating the structure and parameters of a sparse DAG from possibly high-dimensional data, and show how standard numerical solvers can be used to find stationary points.\n",
    "\n",
    "- **Empirical effectiveness**:  \n",
    "  We demonstrate the effectiveness of the resulting method in empirical evaluations against existing state-of-the-art approaches. (See Figure 1 for a quick illustration and Section 5 for details.)\n",
    "\n",
    "- **Comparison with global minimizer**:  \n",
    "  We compare our output to the exact global minimizer [12], and show that our method attains scores that are comparable to the globally optimal score in practice, although our methods are only guaranteed to find stationary points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d116783f",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The basic DAG learning problem is formulated as follows:  \n",
    "Let $X \\in \\mathbb{R}^{n \\times d}$ be a data matrix consisting of $n$ i.i.d. observations of the random vector $X = (X_1, \\dots, X_d)$. Let $\\mathcal{D}$ denote the (discrete) space of DAGs $G = (V, E)$ on $d$ nodes. Given $X$, we seek to learn a DAG $G \\in \\mathcal{D}$ (a *Bayesian network*) for the joint distribution $P(X)$ [22, 42].  \n",
    "\n",
    "We model $X$ via a **structural equation model (SEM)** defined by a weighted adjacency matrix $W \\in \\mathbb{R}^{d \\times d}$. Instead of operating on the discrete space $\\mathcal{D}$, we work in the continuous space $\\mathbb{R}^{d \\times d}$.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Score Functions and SEM\n",
    "\n",
    "Any $W \\in \\mathbb{R}^{d \\times d}$ defines a graph on $d$ nodes via:  \n",
    "$$\n",
    "A(W)_{ij} = \\begin{cases} \n",
    "1 & \\text{if } w_{ij} \\neq 0, \\\\\n",
    "0 & \\text{otherwise},\n",
    "\\end{cases}\n",
    "$$  \n",
    "where $A(W)$ is the adjacency matrix of the directed graph $G(W)$. We abuse notation by treating $W$ itself as a weighted graph.\n",
    "\n",
    "### Linear SEM\n",
    "For $W = [w_1 \\mid \\cdots \\mid w_d]$, the linear SEM is:  \n",
    "$$\n",
    "X_j = w_j^\\top X + z_j \\quad (j = 1, \\dots, d),\n",
    "$$  \n",
    "where:\n",
    "- $X = (X_1, \\dots, X_d)$ is a random vector,\n",
    "- $z = (z_1, \\dots, z_d)$ is a noise vector (not assumed Gaussian).\n",
    "\n",
    "### Generalized Linear Model (GLM) Extension\n",
    "For non-linear dependencies, we model:  \n",
    "$$\n",
    "\\mathbb{E}[X_j \\mid X_{\\text{pa}(X_j)}] = f(w_j^\\top X),\n",
    "$$  \n",
    "where $f$ is a link function (e.g., logistic for $X_j \\in \\{0, 1\\}$).\n",
    "\n",
    "### Focus: Linear SEM + Least Squares\n",
    "This paper focuses on:\n",
    "1. **Linear SEMs**,\n",
    "2. **Least-squares loss**:  \n",
    "$$\n",
    "\\ell(W; X) = \\frac{1}{2n} \\|X - XW\\|_F^2,\n",
    "$$  \n",
    "where $\\|\\cdot\\|_F$ is the Frobenius norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab7859",
   "metadata": {},
   "source": [
    "## 2.2 Previous Work\n",
    "\n",
    "Traditional score-based learning optimizes a discrete score $Q: \\mathcal{D} \\to \\mathbb{R}$ over the set of DAGs $\\mathcal{D}$. This differs from our score $F(W)$ defined on $\\mathbb{R}^{d \\times d}$. The combinatorial optimization problem is:\n",
    "\n",
    "$$\n",
    "\\min_{G} Q(G) \\quad \\text{subject to} \\quad G \\in \\mathcal{D}\n",
    "$$\n",
    "\n",
    "**Key Score Functions**:\n",
    "- BDe(u) [20]\n",
    "- BGe [23] \n",
    "- BIC [10]\n",
    "- MDL [6]\n",
    "\n",
    "### Computational Challenges\n",
    "- Problem (4) is **NP-hard** [8, 11] due to the nonconvex, combinatorial nature.\n",
    "- Acyclicity constraint leads to superexponential growth in possible structures with $d$ [32].\n",
    "\n",
    "### Existing Approaches\n",
    "1. **Global Optimality Methods** (for small problems):\n",
    "   - [12, 13, 29, 39, 40, 47]\n",
    "\n",
    "2. **Approximate Algorithms**:\n",
    "   - *Order search*: Trades acyclicity for $d!$ ordering search [30, 34‚Äì36, 43]\n",
    "   - *Greedy search*: Explicit acyclicity checks per edge addition [9, 20, 31]  \n",
    "   - *Coordinate descent*: [2, 16, 18]\n",
    "\n",
    "3. **Alternative Methods**:\n",
    "   - Constraint-based [41, 42]\n",
    "   - Hybrid [17, 44]  \n",
    "   - Bayesian [14, 27, 51]\n",
    "\n",
    "### Comparison to Undirected Graphs\n",
    "Undirected graph learning (Markov networks) saw breakthroughs when reformulated as:\n",
    "\n",
    "$$\n",
    "\\text{Convex program over real, symmetric matrices} \\quad \\text{[4, 48]}\n",
    "$$\n",
    "\n",
    "**Why It Worked**:\n",
    "- Closed-form, tractable program enabled optimization techniques [15, 21, 37].\n",
    "\n",
    "**DAG Learning Gap**:\n",
    "- No analogous reformulation exists for (4) due to acyclicity intractability.\n",
    "\n",
    "**Our Goal**:\n",
    "Propose a **continuous program** for DAG learning via a *smooth acyclicity characterization* (introduced next)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}